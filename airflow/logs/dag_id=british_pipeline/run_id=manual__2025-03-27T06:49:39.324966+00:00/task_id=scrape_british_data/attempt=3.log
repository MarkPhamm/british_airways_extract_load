[2025-03-27T07:02:32.834+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: british_pipeline.scrape_british_data manual__2025-03-27T06:49:39.324966+00:00 [queued]>
[2025-03-27T07:02:32.839+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: british_pipeline.scrape_british_data manual__2025-03-27T06:49:39.324966+00:00 [queued]>
[2025-03-27T07:02:32.839+0000] {taskinstance.py:1359} INFO - Starting attempt 3 of 4
[2025-03-27T07:02:32.849+0000] {taskinstance.py:1380} INFO - Executing <Task(BashOperator): scrape_british_data> on 2025-03-27 06:49:39.324966+00:00
[2025-03-27T07:02:32.852+0000] {standard_task_runner.py:57} INFO - Started process 208 to run task
[2025-03-27T07:02:32.855+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'british_pipeline', 'scrape_british_data', 'manual__2025-03-27T06:49:39.324966+00:00', '--job-id', '42', '--raw', '--subdir', 'DAGS_FOLDER/main_dag.py', '--cfg-path', '/tmp/tmptz666c16']
[2025-03-27T07:02:32.857+0000] {standard_task_runner.py:85} INFO - Job 42: Subtask scrape_british_data
[2025-03-27T07:02:32.890+0000] {task_command.py:415} INFO - Running <TaskInstance: british_pipeline.scrape_british_data manual__2025-03-27T06:49:39.324966+00:00 [running]> on host fec71cbbe55f
[2025-03-27T07:02:32.944+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='british_pipeline' AIRFLOW_CTX_TASK_ID='scrape_british_data' AIRFLOW_CTX_EXECUTION_DATE='2025-03-27T06:49:39.324966+00:00' AIRFLOW_CTX_TRY_NUMBER='3' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-03-27T06:49:39.324966+00:00'
[2025-03-27T07:02:32.945+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-03-27T07:02:32.946+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /opt/***/tasks/scraper_extract/scraper.py']
[2025-03-27T07:02:32.951+0000] {subprocess.py:86} INFO - Output:
[2025-03-27T07:02:33.551+0000] {subprocess.py:93} INFO - 2025-03-27 07:02:33,551 - INFO - Scraping page 1...
[2025-03-27T07:02:36.123+0000] {subprocess.py:93} INFO - 2025-03-27 07:02:36,123 - INFO - Scraping page 2...
[2025-03-27T07:02:38.238+0000] {subprocess.py:93} INFO - 2025-03-27 07:02:38,237 - INFO - Scraping page 3...
[2025-03-27T07:02:40.482+0000] {subprocess.py:93} INFO - 2025-03-27 07:02:40,482 - INFO - Scraping page 4...
[2025-03-27T07:02:42.802+0000] {subprocess.py:93} INFO - 2025-03-27 07:02:42,802 - INFO - Scraping page 5...
[2025-03-27T07:02:45.191+0000] {subprocess.py:93} INFO - 2025-03-27 07:02:45,191 - INFO - Scraping page 6...
[2025-03-27T07:02:47.682+0000] {subprocess.py:93} INFO - 2025-03-27 07:02:47,682 - INFO - Scraping page 7...
[2025-03-27T07:02:50.073+0000] {subprocess.py:93} INFO - 2025-03-27 07:02:50,073 - INFO - Scraping page 8...
[2025-03-27T07:02:51.959+0000] {subprocess.py:93} INFO - 2025-03-27 07:02:51,959 - INFO - Scraping page 9...
[2025-03-27T07:02:54.317+0000] {subprocess.py:93} INFO - 2025-03-27 07:02:54,317 - INFO - Scraping page 10...
[2025-03-27T07:02:56.266+0000] {subprocess.py:93} INFO - 2025-03-27 07:02:56,266 - INFO - Scraping page 11...
[2025-03-27T07:02:58.261+0000] {subprocess.py:93} INFO - 2025-03-27 07:02:58,261 - INFO - Scraping page 12...
[2025-03-27T07:03:00.263+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:00,262 - INFO - Scraping page 13...
[2025-03-27T07:03:02.367+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:02,367 - INFO - Scraping page 14...
[2025-03-27T07:03:04.672+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:04,672 - INFO - Scraping page 15...
[2025-03-27T07:03:07.053+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:07,053 - INFO - Scraping page 16...
[2025-03-27T07:03:09.120+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:09,120 - INFO - Scraping page 17...
[2025-03-27T07:03:11.571+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:11,571 - INFO - Scraping page 18...
[2025-03-27T07:03:13.462+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:13,462 - INFO - Scraping page 19...
[2025-03-27T07:03:15.875+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:15,875 - INFO - Scraping page 20...
[2025-03-27T07:03:18.621+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:18,621 - INFO - Scraping page 21...
[2025-03-27T07:03:22.137+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:22,137 - INFO - Scraping page 22...
[2025-03-27T07:03:24.330+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:24,330 - INFO - Scraping page 23...
[2025-03-27T07:03:26.412+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:26,412 - INFO - Scraping page 24...
[2025-03-27T07:03:29.026+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:29,026 - INFO - Scraping page 25...
[2025-03-27T07:03:31.419+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:31,418 - INFO - Scraping page 26...
[2025-03-27T07:03:33.756+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:33,756 - INFO - Scraping page 27...
[2025-03-27T07:03:35.819+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:35,819 - INFO - Scraping page 28...
[2025-03-27T07:03:37.773+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:37,773 - INFO - Scraping page 29...
[2025-03-27T07:03:40.247+0000] {subprocess.py:93} INFO - 2025-03-27 07:03:40,247 - INFO - Scraping page 30...
[2025-03-27T07:03:42.315+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-03-27T07:03:42.315+0000] {subprocess.py:93} INFO -   File "/opt/***/tasks/scraper_extract/scraper.py", line 145, in <module>
[2025-03-27T07:03:42.316+0000] {subprocess.py:93} INFO -     scraper.scrape()
[2025-03-27T07:03:42.316+0000] {subprocess.py:93} INFO -   File "/opt/***/tasks/scraper_extract/scraper.py", line 52, in scrape
[2025-03-27T07:03:42.316+0000] {subprocess.py:93} INFO -     self.save_to_csv(df)
[2025-03-27T07:03:42.316+0000] {subprocess.py:93} INFO -   File "/opt/***/tasks/scraper_extract/scraper.py", line 140, in save_to_csv
[2025-03-27T07:03:42.316+0000] {subprocess.py:93} INFO -     df.to_csv(self.output_path, index=False)
[2025-03-27T07:03:42.316+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/pandas/util/_decorators.py", line 211, in wrapper
[2025-03-27T07:03:42.323+0000] {subprocess.py:93} INFO -     return func(*args, **kwargs)
[2025-03-27T07:03:42.323+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/pandas/core/generic.py", line 3720, in to_csv
[2025-03-27T07:03:42.324+0000] {subprocess.py:93} INFO -     return DataFrameRenderer(formatter).to_csv(
[2025-03-27T07:03:42.324+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/pandas/util/_decorators.py", line 211, in wrapper
[2025-03-27T07:03:42.325+0000] {subprocess.py:93} INFO -     return func(*args, **kwargs)
[2025-03-27T07:03:42.325+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/pandas/io/formats/format.py", line 1189, in to_csv
[2025-03-27T07:03:42.325+0000] {subprocess.py:93} INFO -     csv_formatter.save()
[2025-03-27T07:03:42.325+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/pandas/io/formats/csvs.py", line 241, in save
[2025-03-27T07:03:42.326+0000] {subprocess.py:93} INFO -     with get_handle(
[2025-03-27T07:03:42.326+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/pandas/io/common.py", line 856, in get_handle
[2025-03-27T07:03:42.326+0000] {subprocess.py:93} INFO -     handle = open(
[2025-03-27T07:03:42.326+0000] {subprocess.py:93} INFO - PermissionError: [Errno 13] Permission denied: '/opt/***/data/raw_data.csv'
[2025-03-27T07:03:42.416+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-03-27T07:03:42.424+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-03-27T07:03:42.426+0000] {taskinstance.py:1398} INFO - Marking task as UP_FOR_RETRY. dag_id=british_pipeline, task_id=scrape_british_data, execution_date=20250327T064939, start_date=20250327T070232, end_date=20250327T070342
[2025-03-27T07:03:42.435+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 42 for task scrape_british_data (Bash command failed. The command returned a non-zero exit code 1.; 208)
[2025-03-27T07:03:42.467+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-03-27T07:03:42.498+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
